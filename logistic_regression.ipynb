{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"logistic_regression.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1MNjfrY7toSK"},"outputs":[],"source":["# In this notebook we will optimize a logistic regression classifier\n","# with gradient descent\n","\n","# We will continue working with the tinysol dataset\n","\n","# install mirdata on the colab shell\n","!pip install mirdata\n","\n","# now import mirdata\n","import mirdata"]},{"cell_type":"code","source":["# now we can initialize the tinysol dataset and (down)load it\n","tinysol = mirdata.initialize('tinysol')\n","tinysol.download()"],"metadata":{"id":"CiFKhQVxtqB-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# let's load a few libraries that we will need\n","import librosa\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","###############################################################\n","# we will work with two features                              #\n","#                                                             #\n","# we will extract these features from all relevant datapoints #\n","###############################################################\n","\n","# get all the track_ids\n","all_tracks = tinysol.track_ids\n","\n","# create a \"data\" matrix with two columns, whose rows correspond to datapoints\n","\n","# The \"data\" matrix should ONLY contain Violin and Bass Tuba datapoints.\n","\n","# Extract the two best features that you think will help you \"separate\"\n","# these two musical instruments.\n","\n","# Also create a \"labels\" matrix with only one column.\n","# Every entry in \"labels\" should either be a 0 or a 1, depending on\n","# whether the corresponding row of \"data\" contains a Violin or a Bass Tuba \n","# datapoint. 0==\"Vn\" and 1==\"BTb\"\n","\n","data = []\n","labels = []\n","for t in all_tracks:  \n","  if # your code here\n","    x,sr = # your code here\n","    # feature 1. \n","    f1 = # your code here\n","    # feature 2. \n","    f2 = # your code here\n","    \n","    # concatenate them\n","    data.append( # your code here\n","    labels.append( # your code here\n","\n","data = np.array(data)\n","labels = np.array(labels)\n","\n","print(\"\\nThe shape of 'data' is \", data.shape)\n","print(\"The shape of 'labels' is \", labels.shape,\"\\n\")\n","\n","# we can also visualize these features\n","plt.scatter(data[labels==0,0],data[labels==0,1], label='Vn') \n","plt.scatter(data[labels==1,0],data[labels==1,1], label='BTb')\n","plt.xlabel('Feature No. 1')\n","plt.ylabel('Feature No. 2')\n","plt.legend()\n","plt.show()\n","\n","# Q: Which two features did you pick and why?\n","# A: \n","\n","# Finally, make the x and y axis of the plot have labels that\n","# indicate the feature you extracted"],"metadata":{"id":"H-ZYUUnCUxaj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now randomly select ~5% of the data and set it apart as test set\n","# Hint: you can use the np.random.choice function (with replace=False) \n","# and use the first ~5% of its output to index out the test set\n","\n","all_idx = np.random.choice(# your code here\n","\n","data_ts = data[# your code here\n","labels_ts = labels[# your code here\n","data_tr = data[# your code here\n","labels_tr = labels[# your code here\n","\n","print(\"The shape of the training data is \", data_tr.shape)\n","print(\"The shape of the training labels is \", labels_tr.shape)\n","print(\"The shape of the testing data is \", data_ts.shape)\n","print(\"The shape of the testing labels is \", labels_ts.shape)"],"metadata":{"id":"hZvqBkhZt-BQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some libraries we will use\n","! pip install sklearn\n","from sklearn.model_selection import KFold\n","from IPython import display\n","\n","\n","# now let's standardize our training features to be zero-mean and unit variance\n","mu_tr = np.mean(data_tr, axis=0)\n","std_tr = np.std(data_tr,axis=0)\n","data_tr = (data_tr - mu_tr)/std_tr\n","\n","# Make sure you only run this cell once. Why? "],"metadata":{"id":"8yuhIy7iw2h7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Here's the main body of this homework\n","\n","# we will use k-fold cross validation to optimize a simple logistic regression\n","# model using gradient descent. \n","\n","# let's start by defining our independent (x) and dependent (y) variables\n","# hint: x will have the features and y will have the 0 or 1 labels\n","\n","# Q: what do you expect the shape of x and y should be?\n","# A:\n","\n","x = # your code here\n","y = # your code here\n","\n","# Now define the learning variables for the optimization algorithm\n","epochs = # your code here\n","lr = # your code here\n","# Q: How can we know how many epochs we should let the model learn?\n","\n","# we are ready to start the optimization routine\n","# using sklearn.Kfold, we will split the data into 5 folds\n","# and we will use each fold as validation set in a for loop\n","n_splits = 5\n","kf = KFold(n_splits=n_splits)\n","# we will also save the slope and bias terms that we find\n","# for each fold in a list\n","ws = []\n","bs = []\n","all_Jtrs = []\n","all_Jvls = []\n","for ifold, (tr_idx, vl_idx) in enumerate(kf.split(x)):\n","\n","  print(\"\\n#######################\")\n","  print(\"# training fold No. {} #\".format(ifold+1))\n","  print(\"#######################\\n\")  \n","\n","  # organize the data into training and validation splits\n","  x_tr, x_vl = x[tr_idx], x[vl_idx]\n","  y_tr, y_vl = y[tr_idx], y[vl_idx]\n","    \n","  # intialize the slope and bias as\n","  # random numbers drawn from a normal\n","  # distribution with a small variance. Why?\n","  # A:\n","  # (hint: use np.random.randn)\n","  w = # your code here\n","  b = # your code here\n","\n","  # we will repeatedly show the data \n","  # to out gradient descent\n","  # algorithm a few times \n","  Jtrs = []\n","  Jvls = []\n","  for e in range(epochs):\n","    \n","    # compute y_hat with the training data\n","    # Q: what do you expect the initial average y_hat_tr (training) to be? why?\n","    # A:         \n","    y_hat_tr = # your code here\n","    # compute y_hat with the validation data\n","    # Q: what do you expect the initial average y_hat_vl (validation) to be? why?\n","    # A:             \n","    y_hat_vl = # your code here\n","    # Q: what does each value in y_hat (training or validation) mean?\n","    # A:\n","\n","    # compute the loss function with the training data\n","    # Q: what do you expect the initial training loss to be?\n","    # A: \n","    J_tr = # your code here\n","    \n","    # compute the loss function with the validation data\n","    # Q: what do you expect the initial validation loss to be?\n","    # A:     \n","    J_vl = # your code here\n","\n","    # Hint: print the initial y_hat and J values before training the model\n","    # If you do not see the initial values that you expect, you are likely\n","    # doing something wrong and the model will not be able to learn\n","\n","    # save the training and validation loss to visualize at the end\n","    Jtrs.append(J_tr)\n","    Jvls.append(J_vl)\n","\n","    print(\"at epoch {} the training loss J_tr = {:.5f}\".format(e+1,J_tr))\n","    print(\"          and validation loss J_vl = {:.5f}\".format(J_vl))\n","    \n","    # now compute the gradient of w and b\n","    dw = # your code here\n","    db = # your code here\n","\n","    # and update w and b\n","    w = w - lr*dw\n","    b = b - lr*db    \n","\n","  # let's save the fold's optimized parameters and each epoch tr and vl loss\n","  ws.append(w)\n","  bs.append(b)\n","  all_Jtrs.append(Jtrs)\n","  all_Jvls.append(Jvls)    \n","\n","for i in range(n_splits):\n","  print(\"for fold No.\",i+1, \", the optimized parameters were \\nw=\", ws[i], \"\\nb=\",bs[i])"],"metadata":{"id":"6vZvfsEnhcRX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"\\nPlotting the training and validation losses:\\n\")\n","\n","plt.plot(np.array(all_Jtrs).T)\n","plt.title('Training loss (each line is a different fold)')\n","plt.xlabel('Epoch No.')\n","plt.ylabel('J')\n","plt.ylim([0,0.8])\n","plt.show()\n","plt.plot(np.array(all_Jvls).T)\n","plt.title('Validation loss (each line is a different fold)')\n","plt.xlabel('Epoch No.')\n","plt.ylabel('J')\n","plt.ylim([0,0.8])\n","plt.show()\n","plt.title('Loss (mean across folds)')\n","plt.plot(np.mean(np.array(all_Jtrs),axis=0),label='Training')\n","plt.plot(np.mean(np.array(all_Jvls),axis=0),label='Validation')\n","plt.xlabel('Epoch No.')\n","plt.ylabel('J')\n","plt.ylim([0,0.8])\n","plt.legend()\n","plt.show()\n","\n","# Q: how do the losses differ between folds? why do you think this is seen?\n","# A:\n","\n","# Q: how do the losses differ between training and validation splits? why do you think this is seen?\n","# A:\n","\n","# Q: is your model overfit, underfit, or properly optimized? how do you know this?\n","# A:\n","\n","# Q: Did your loss go close to zero? why? Is this good or bad?\n","# A:"],"metadata":{"id":"z9QbDu7H94pX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sklearn\n","\n","####################\n","# model evaluation #\n","####################\n","\n","# now you have a list of 'w' and a list of 'b' parameters\n","# to evaluate the model, we only need one, not five. \n","# Q: how would you go about getting a single 'w' and a single 'b'?\n","# A:\n","\n","w = # your code here\n","b = # your code here\n","\n","# Now, using your optimized 'w' and 'b' calculate the training accuracy by comparing y_hat and y\n","y_hat = # your code here\n","training_accuracy = # your code here\n","print(\"The training acc was: {:.2f}%\\n\".format(training_accuracy))\n","\n","# Now compute a confusion matrix\n","# Hint: use sklearn.metrics.confusion_matrix\n","conf_mat = sklearn.metrics.confusion_matrix(# your code here\n","\n","print(\"The training confusion matrix: \")\n","print(conf_mat)\n","\n","# Now calculate the \"true positive rate\", Q: What does positive stand for here?\n","# Q: Can we calculate the \"true positive rate\" if we are classifying musical\n","# instrument sounds and not whether something is true or false? why?\n","# A:\n","print(\"\\nThe true positive rate:\")\n","print(# your code here\n","# Q: What does the \"true positive rate\" tell us?\n","# A:\n","\n","# Q: How do you calculate the \"true negative rate\"?\n","print(\"\\nThe true negative rate:\")\n","print(# your code here\n","# Q: What does the \"true negative rate\" tell us?\n","# A:\n","\n","# Q: How do you calculate the \"false positive rate\"\n","print(\"\\nThe false positive rate:\")\n","print(# your code here\n","# Q: What does the \"false positive rate\" tell us?\n","# A:\n","\n","# Q: How do you calculate the \"false negative rate\"\n","print(\"\\nThe false negative rate:\")\n","print(# your code here\n","# Q: What does the \"false negative rate\" tell us?\n","# A:\n","\n","# Q: Do you have the same number of Violin and Bass Tuba datapoints in the\n","# training data? \n","# A:\n","# Q: Can this imbalance affect your classification algorithm? How?\n","# A:\n","# HINT: If you balance your training data to have the same number of\n","# datapoints in each class, and you optimize your classification model with \n","# such a dataset, you will likely develop a be more robust model. Why?\n","# A:"],"metadata":{"id":"DviYF-_vvwuL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's also visualize the logistic regression on the training data\n","\n","# Here, \"theta\" is the value that gets negated to be the power of e in the\n","# logistic regression formula. Calculate \"theta\" using all the training datapoints\n","theta = # your code here\n","\n","plt.figure(figsize=(30,10))\n","xaxis = np.linspace(-3,3,100)\n","plt.scatter(theta[y==0],y_hat[y==0],label='Vn')\n","plt.scatter(theta[y==1],y_hat[y==1],label='BTb')\n","plt.plot(xaxis,1/(1+np.exp(-xaxis)))\n","plt.legend()\n","plt.ylim([-0,1])\n","plt.grid()\n","plt.show()\n","\n","# Q: What does this plot tell us about what our model was able to learn?\n","# A: \n","\n","# Q: what is the x axis in this plot?\n","# A:\n","\n","# Q: what is the y axis in this plot?\n","# A:\n","\n","##################################################################################\n","# ATTENTION: DO NOT move to the next cell until you have a well-optimized model. #\n","# continuing without a well-optimized model will ruin the model evaluation.      #\n","##################################################################################"],"metadata":{"id":"OhsjtRs6oDdy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##################################################################\n","# RUN THIS CELL ONLY ONCE WHEN YOU ARE DONE WITH THE CELLS ABOVE #\n","# Running this cell more than once is a violation of             #\n","# international Machine Learning Law                             #\n","##################################################################\n","\n","# now do the same evaluation with the test set\n","\n","# IMPORTANT: you will have to standardize the test data using the training data mu and std\n","# Q: why do we standardize the test data with the training mu and std?\n","# A:\n","\n","data_ts = # your code here\n","\n","x_ts = # your code here\n","y_ts = # your code here\n","\n","# Now, using your optimized 'w' and 'b' calculate the training accuracy by comparing y_hat and y\n","y_hat = # your code here\n","testing_accuracy = # your code here\n","print(\"The testing acc was: {:.2f}%\\n\".format(testing_accuracy))\n","\n","# Now compute a confusion matrix\n","# Hint: use sklearn.metrics.confusion_matrix\n","conf_mat = sklearn.metrics.confusion_matrix(# your code here\n","\n","print(\"The testing confusion matrix: \")\n","print(conf_mat)\n","\n","# Now calculate the \"true positive rate\", where positive means \"Violin\"\n","print(\"\\nThe true positive rate:\")\n","print(# your code here\n","\n","print(\"\\nThe true negative rate:\")\n","print(# your code here\n","\n","print(\"\\nThe false positive rate:\")\n","print(# your code here\n","\n","print(\"\\nThe false negative rate:\")\n","print(# your code here\n"],"metadata":{"id":"GN812LVcx0CM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Q: were your optimized 'w' and 'b' parameters good for the test set data? how do you know if they were or not?\n","# A: \n","\n","# Q: which was lower, the training or the test error? Why?\n","# A: "],"metadata":{"id":"8EmmdNjn3cdh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CHALLENGE (without doing this, the highest score you can earn in this homework is 95/100)\n","\n","# It is possible to carry out binary classification using linear regression\n","# Q: Why do we use logistic regression then? A:\n","\n","# In the cells below, use linear regression to carry out this binary classification task\n","\n","# NOTE: Being able to do this challenge serves as a good check for understanding of\n","# the regression concepts (both linear and logistic non-linear) covered so far in class"],"metadata":{"id":"7D6heKg7_v9V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"TVJ2QGMmKOs4"},"execution_count":null,"outputs":[]}]}