{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lr.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MNjfrY7toSK"
      },
      "outputs": [],
      "source": [
        "# In this notebook we will get optimize a logistic regression classifier\n",
        "# with gradient descent\n",
        "\n",
        "# We will continue working with the tinysol dataset\n",
        "\n",
        "# install mirdata on the colab shell\n",
        "!pip install mirdata\n",
        "\n",
        "# now import mirdata\n",
        "import mirdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can initialize the tinysol dataset and (down)load it\n",
        "tinysol = mirdata.initialize('tinysol')\n",
        "tinysol.download()"
      ],
      "metadata": {
        "id": "CiFKhQVxtqB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3d98ce2-9eab-4d0a-fa14-cf5a4bd6df9c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Downloading ['audio', 'annotations'] to /root/mir_datasets/tinysol\n",
            "INFO: [audio] downloading TinySOL.tar.gz\n",
            "INFO: /root/mir_datasets/tinysol/audio/TinySOL.tar.gz already exists and will not be downloaded. Rerun with force_overwrite=True to delete this file and force the download.\n",
            "INFO: [annotations] downloading TinySOL_metadata.csv\n",
            "312kB [00:06, 51.2kB/s]                          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's load a few libraries that we will need\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "###############################################################\n",
        "# we will work with two features                              #\n",
        "#                                                             #\n",
        "# we will extract these features from all relevant datapoints #\n",
        "###############################################################\n",
        "\n",
        "# get all the track_ids\n",
        "all_tracks = tinysol.track_ids\n",
        "\n",
        "# create a \"data\" matrix with two columns, whose rows correspond to datapoints\n",
        "\n",
        "# The \"data\" matrix should ONLY contain Violin and Bass Tuba datapoints.\n",
        "\n",
        "# Extract the two best features that you think will help you \"separate\"\n",
        "# these two musical instruments.\n",
        "\n",
        "# Also crate a \"labels\" matrix with only one column.\n",
        "# Every entry in \"labels\" should either be a 0 or a 1, depending on\n",
        "# whether the corresponding row of \"data\" contains a Violin or a Bass Tuba \n",
        "# datapoint. 0==\"Vn\" and 1==\"BTb\"\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "for t in all_tracks:  \n",
        "  if # your code here\n",
        "    x,sr = # your code here\n",
        "    # feature 1. \n",
        "    f1 = # your code here\n",
        "    # feature 2. \n",
        "    f2 = # your code here\n",
        "    \n",
        "    # concatenate them\n",
        "    data.append( # your code here\n",
        "    labels.append( # your code here\n",
        "\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(\"\\nThe shape of 'data' is \", data.shape)\n",
        "print(\"The shape of 'labels' is \", labels.shape,\"\\n\")\n",
        "\n",
        "# we can also visualize these features\n",
        "plt.scatter(data[labels==0,0],data[labels==0,1], label='Vn') \n",
        "plt.scatter(data[labels==1,0],data[labels==1,1], label='BTb')\n",
        "plt.xlabel('Feture No. 1')\n",
        "plt.ylabel('Feture No. 2')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Q: Which two features did you pick and why?\n",
        "# A: \n",
        "\n",
        "# make the x and y axis of the plot have labels that\n",
        "# indicate the feature you extracted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "H-ZYUUnCUxaj",
        "outputId": "44b0bfd4-e5d2-448b-86a7-1c7761dff561"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The shape of 'data' is  (392, 2)\n",
            "The shape of 'labels' is  (392,) \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now randomly select ~5% of the data and set it apart as test set\n",
        "# Hint: you can use the np.random.choice function (with replace=False) \n",
        "# and use the first ~5% of its output to index out the test set\n",
        "\n",
        "all_idx = np.random.choice(# your code here\n",
        "\n",
        "data_ts = data[# your code here\n",
        "labels_ts = labels[# your code here\n",
        "data_tr = data[# your code here\n",
        "labels_tr = labels[# your code here\n",
        "\n",
        "print(\"The shape of the training data is \", data_tr.shape)\n",
        "print(\"The shape of the training labels is \", labels_tr.shape)\n",
        "print(\"The shape of the testing data is \", data_ts.shape)\n",
        "print(\"The shape of the testing labels is \", labels_ts.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZvqBkhZt-BQ",
        "outputId": "8074c520-96ea-48da-deec-f94baf89ddb3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the training data is  (372, 2)\n",
            "The shape of the training labels is  (372,)\n",
            "The shape of the testing data is  (20, 2)\n",
            "The shape of the testing labels is  (20,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# some libraries we will use\n",
        "! pip install sklearn\n",
        "from sklearn.model_selection import KFold\n",
        "from IPython import display\n",
        "\n",
        "\n",
        "# now let's standardize our training features to be zero-mean and unit variance\n",
        "mu_tr = np.mean(data_tr, axis=0)\n",
        "std_tr = np.std(data_tr,axis=0)\n",
        "data_tr = (data_tr - mu_tr)/std_tr\n",
        "\n",
        "# Make sure you only run this cell once. Why? "
      ],
      "metadata": {
        "id": "8yuhIy7iw2h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's the main body of this homework\n",
        "\n",
        "# we will use k-fold cross validation to optimize a simple logistic regression\n",
        "# model using gradient descent. \n",
        "\n",
        "# let's start by defining our independent (x) and dependent (y) variables\n",
        "# hint: x will have the featues and y will have the 0 or 1 labels\n",
        "\n",
        "# Q: what do you expect the shape of x and y should be?\n",
        "# A:\n",
        "\n",
        "x = # your code here\n",
        "y = # your code here\n",
        "\n",
        "# Now define the learning variables for the optimization algorithm\n",
        "epochs = # your code here\n",
        "lr = # your code here\n",
        "# Q: How can we know how many epochs we should let the model learn?\n",
        "\n",
        "# we are ready to start the optimization routine\n",
        "# using sklearn.Kfold, we will split the data into 5 folds\n",
        "# and we will use each fold as validation set in a for loop\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits)\n",
        "# we will also save the slope and bias terms that we find\n",
        "# for each fold in a list\n",
        "ws = []\n",
        "bs = []\n",
        "all_Jtrs = []\n",
        "all_Jvls = []\n",
        "for ifold, (tr_idx, vl_idx) in enumerate(kf.split(x)):\n",
        "\n",
        "  print(\"\\n#######################\")\n",
        "  print(\"# training fold No. {} #\".format(ifold+1))\n",
        "  print(\"#######################\\n\")  \n",
        "\n",
        "  # organize the data into training and validation splits\n",
        "  x_tr, x_vl = x[tr_idx], x[vl_idx]\n",
        "  y_tr, y_vl = y[tr_idx], y[vl_idx]\n",
        "    \n",
        "  # intialize the slope and bias as\n",
        "  # random numbers drawn from a normal\n",
        "  # distribution with a small variance. Why?\n",
        "  # A:\n",
        "  # (hint: use np.random.randn)\n",
        "  w = # your code here\n",
        "  b = # your code here\n",
        "\n",
        "  # we will repeatedly show the data \n",
        "  # to out gradient descent\n",
        "  # algorithm a few times \n",
        "  Jtrs = []\n",
        "  Jvls = []\n",
        "  for e in range(epochs):\n",
        "    \n",
        "    # compute y_hat with the training data\n",
        "    # Q: what do you expect the initial average y_hat_tr (training) to be? why?\n",
        "    # A:         \n",
        "    y_hat_tr = # your code here\n",
        "    # compute y_hat with the validation data\n",
        "    # Q: what do you expect the initial average y_hat_vl (validation) to be? why?\n",
        "    # A:             \n",
        "    y_hat_vl = # your code here\n",
        "    # Q: what does each value in y_hat (training or validation) mean?\n",
        "    # A:\n",
        "\n",
        "    # compute the loss function with the training data\n",
        "    # Q: what do you expect the initial training loss to be?\n",
        "    # A: \n",
        "    J_tr = # your code here\n",
        "    \n",
        "    # compute the loss function with the validation data\n",
        "    # Q: what do you expect the initial validation loss to be?\n",
        "    # A:     \n",
        "    J_vl = # your code here\n",
        "\n",
        "    # Hint: print the initial y_hat and J values before training the model\n",
        "    # If you do not see the initial values that you expect, you are likely\n",
        "    # doing something wrong and the model will not be able to learn\n",
        "\n",
        "    # save the training and validation loss to visualize at the end\n",
        "    Jtrs.append(J_tr)\n",
        "    Jvls.append(J_vl)\n",
        "\n",
        "    print(\"at epoch {} the training loss J_tr = {:.5f}\".format(e+1,J_tr))\n",
        "    print(\"          and validation loss J_vl = {:.5f}\".format(J_vl))\n",
        "    \n",
        "    # now compute the gradient of w and b\n",
        "    dw = # your code here\n",
        "    db = # your code here\n",
        "\n",
        "    # and update w and b\n",
        "    w = w - lr*dw\n",
        "    b = b - lr*db    \n",
        "\n",
        "  # let's save the fold's optimized parameters and each epoch tr and vl loss\n",
        "  ws.append(w)\n",
        "  bs.append(b)\n",
        "  all_Jtrs.append(Jtrs)\n",
        "  all_Jvls.append(Jvls)    \n",
        "\n",
        "for i in range(n_splits):\n",
        "  print(\"for fold No.\",i+1, \", the optimized parameters were \\nw=\", ws[i], \"\\nb=\",bs[i])"
      ],
      "metadata": {
        "id": "6vZvfsEnhcRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPlotting the training and validation losses:\\n\")\n",
        "\n",
        "plt.plot(np.array(all_Jtrs).T)\n",
        "plt.title('Training loss (each line is a different fold)')\n",
        "plt.xlabel('Epoch No.')\n",
        "plt.ylabel('J')\n",
        "plt.ylim([0,0.8])\n",
        "plt.show()\n",
        "plt.plot(np.array(all_Jvls).T)\n",
        "plt.title('Validation loss (each line is a different fold)')\n",
        "plt.xlabel('Epoch No.')\n",
        "plt.ylabel('J')\n",
        "plt.ylim([0,0.8])\n",
        "plt.show()\n",
        "plt.title('Loss (mean across folds)')\n",
        "plt.plot(np.mean(np.array(all_Jtrs),axis=0),label='Training')\n",
        "plt.plot(np.mean(np.array(all_Jvls),axis=0),label='Validation')\n",
        "plt.xlabel('Epoch No.')\n",
        "plt.ylabel('J')\n",
        "plt.ylim([0,0.8])\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Q: how do the losses differ between folds? why do you think this is seen?\n",
        "# A:\n",
        "\n",
        "# Q: how do the losses differ between training and validation splits? why do you think this is seen?\n",
        "# A:\n",
        "\n",
        "# Q: is your model overfit, underfit, or properly optimized? how do you know this?\n",
        "# A:\n",
        "\n",
        "# Q: Did your loss go close to zero? why? Is this good or bad?\n",
        "# A:"
      ],
      "metadata": {
        "id": "z9QbDu7H94pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "\n",
        "####################\n",
        "# model evaluation #\n",
        "####################\n",
        "\n",
        "# now you have a list of 'w' and a list of 'b' parameters\n",
        "# to evaluate the model, we only need one, not five. \n",
        "# Q: how would you go about getting a single 'w' and a single 'b'?\n",
        "# A:\n",
        "\n",
        "w = # your code here\n",
        "b = # your code here\n",
        "\n",
        "# Now, using your optimized 'w' and 'b' calculate the training accuracy by comparing y_hat and y\n",
        "y_hat = # your code here\n",
        "training_accuracy = # your code here\n",
        "print(\"The training acc was: {:.2f}%\\n\".format(training_accuracy))\n",
        "\n",
        "# Now compute a confusion matrix\n",
        "# Hint: use sklearn.metrics.confusion_matrix\n",
        "conf_mat = sklearn.metrics.confusion_matrix(# your code here\n",
        "\n",
        "print(\"The training confusion matrix: \")\n",
        "print(conf_mat)\n",
        "\n",
        "# Now calculate the \"true positive rate\", Q: What does positive stand for here?\n",
        "# Q: Can we calculate the \"true positive rate\" if we are classifying musical\n",
        "# instrument sounds and not whether something is true or false? why?\n",
        "# A:\n",
        "print(\"\\nThe true positive rate:\")\n",
        "print(# your code here\n",
        "# Q: What does the \"true positive rate\" tell us?\n",
        "# A:\n",
        "\n",
        "# Q: How do you calculate the \"true negative rate\"?\n",
        "print(\"\\nThe true negative rate:\")\n",
        "print(# your code here\n",
        "# Q: What does the \"true negative rate\" tell us?\n",
        "# A:\n",
        "\n",
        "# Q: How do you calculate the \"false positive rate\"\n",
        "print(\"\\nThe false positive rate:\")\n",
        "print(# your code here\n",
        "# Q: What does the \"false positive rate\" tell us?\n",
        "# A:\n",
        "\n",
        "# Q: How do you calculate the \"false negative rate\"\n",
        "print(\"\\nThe false negative rate:\")\n",
        "print(# your code here\n",
        "# Q: What does the \"false negative rate\" tell us?\n",
        "# A:\n",
        "\n",
        "# Q: Do you have the same number of Violin and Bass Tuba datapoints in the\n",
        "# training data? \n",
        "# A:\n",
        "# Q: Can this imbalance affect your classification algorithm? How?\n",
        "# A:\n",
        "# HINT: If you balance your training data to have the same number of\n",
        "# datapoints in each class, and you optimize your classification model with \n",
        "# such a dataset, you will likely develop a be more robust model. Why?\n",
        "# A:"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DviYF-_vvwuL",
        "outputId": "98e804f0-ec95-48e3-e6ae-ef5ce736b33b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training acc was: 94.09%\n",
            "\n",
            "The training confusion matrix: \n",
            "[[273   0]\n",
            " [ 22  77]]\n",
            "\n",
            "The true positive rate:\n",
            "100.0 %\n",
            "\n",
            "The true negative rate:\n",
            "77.77777777777777 %\n",
            "\n",
            "The false positive rate:\n",
            "22.22222222222222 %\n",
            "\n",
            "The false negative rate:\n",
            "0.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's also visualize the logistic regression on the training data\n",
        "\n",
        "# Here, \"theta\" is the value that gets negated to be the power of e in the\n",
        "# logistic regression formula. Calculate \"theta\" using all the training datapoints\n",
        "theta = # your code here\n",
        "\n",
        "plt.figure(figsize=(30,10))\n",
        "xaxis = np.linspace(-3,3,100)\n",
        "plt.scatter(theta[y==0],y_hat[y==0],label='Vn')\n",
        "plt.scatter(theta[y==1],y_hat[y==1],label='BTb')\n",
        "plt.plot(xaxis,1/(1+np.exp(-xaxis)))\n",
        "plt.legend()\n",
        "plt.ylim([-0,1])\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Q: What does this plot tell us about what our model was able to learn?\n",
        "# A: \n",
        "\n",
        "# Q: what is the x axis in this plot?\n",
        "# A:\n",
        "\n",
        "# Q: what is the y axis in this plot?\n",
        "# A:\n",
        "\n",
        "##################################################################################\n",
        "# ATTENTION: DO NOT move to the next cell until you have a well-optimized model. #\n",
        "# continuing without a well-optimized model will ruin the model evaluation.      #\n",
        "##################################################################################"
      ],
      "metadata": {
        "id": "OhsjtRs6oDdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################################\n",
        "# RUN THIS CELL ONLY ONCE WHEN YOU ARE DONE WITH THE CELLS ABOVE #\n",
        "# Running this cell more than once is a violation of             #\n",
        "# international Machine Learning Law                             #\n",
        "##################################################################\n",
        "\n",
        "# now do the same evaluation with the test set\n",
        "\n",
        "# IMPORTANT: you will have to standardize the test data using the training data mu and std\n",
        "# Q: why do we standardize the test data with the training mu and std?\n",
        "# A:\n",
        "\n",
        "data_ts = # your code here\n",
        "\n",
        "x_ts = # your code here\n",
        "y_ts = # your code here\n",
        "\n",
        "# Now, using your optimized 'w' and 'b' calculate the training accuracy by comparing y_hat and y\n",
        "y_hat = # your code here\n",
        "testing_accuracy = # your code here\n",
        "print(\"The testing acc was: {:.2f}%\\n\".format(testing_accuracy))\n",
        "\n",
        "# Now compute a confusion matrix\n",
        "# Hint: use sklearn.metrics.confusion_matrix\n",
        "conf_mat = sklearn.metrics.confusion_matrix(# your code here\n",
        "\n",
        "print(\"The testing confusion matrix: \")\n",
        "print(conf_mat)\n",
        "\n",
        "# Now calculate the \"true positive rate\", where positive means \"Violin\"\n",
        "print(\"\\nThe true positive rate:\")\n",
        "print(# your code here\n",
        "\n",
        "print(\"\\nThe true negative rate:\")\n",
        "print(# your code here\n",
        "\n",
        "print(\"\\nThe false positive rate:\")\n",
        "print(# your code here\n",
        "\n",
        "print(\"\\nThe false negative rate:\")\n",
        "print(# your code here\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN812LVcx0CM",
        "outputId": "352f86e7-60b8-4079-9f04-0b233ebe10d7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The testing acc was: 100.00%\n",
            "\n",
            "The testing confusion matrix: \n",
            "[[17  0]\n",
            " [ 0  3]]\n",
            "\n",
            "The true positive rate:\n",
            "100.0 %\n",
            "\n",
            "The true negative rate:\n",
            "100.0 %\n",
            "\n",
            "The false positive rate:\n",
            "0.0 %\n",
            "\n",
            "The false negative rate:\n",
            "0.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q: were your optimized 'w' and 'b' parameters good for the test set data? how do you know if they were or not?\n",
        "# A: \n",
        "\n",
        "# Q: which was lower, the training or the test error? Why?\n",
        "# A: "
      ],
      "metadata": {
        "id": "8EmmdNjn3cdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHALLENGE (without doing this, the highest score you can earn in this homework is 95/100)\n",
        "\n",
        "# It is possible to carry out binary classification using linear regression\n",
        "# Q: Why do we use logistic regression then? A:\n",
        "\n",
        "# In the cells below, use linear regression to carry out this binary classification task\n",
        "\n",
        "# NOTE: Being able to do this challenge serves as a good check for understanding of\n",
        "# the regression concepts (both linear and logistic non-linear) covered so far in class"
      ],
      "metadata": {
        "id": "7D6heKg7_v9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TVJ2QGMmKOs4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
