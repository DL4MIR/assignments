{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn.ipynb","provenance":[{"file_id":"https://github.com/dl4genaudio/assignments/blob/main/cnn.ipynb","timestamp":1658968123934}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZfDOqYceIJnz"},"outputs":[],"source":["# In this notebook you will build a CNN and train it to classify 10 different \n","# musical genres\n","\n","# Fot this, we will use the GTZAN dataset hosted on Kaggle: https://www.kaggle.com/datasets/carlthome/gtzan-genre-collection\n","# see \"Musical genre classification of audio signals \" by G. Tzanetakis and P. Cook"]},{"cell_type":"code","source":["# mount your Google drive so that you only have to download the data only once\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aKZDe3WDIWdU","outputId":"2a70229c-c1b0-4a55-aec1-3464a359d19e","executionInfo":{"status":"ok","timestamp":1659131273998,"user_tz":420,"elapsed":920,"user":{"displayName":"Camille Noufi","userId":"04628122647747040750"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# change the current working directory to be where this .ipynb is located within your Drive\n","# make sure you 'include the full folder path beginning with /content/gdrive/MyDrive'\n","%cd /content/drive/path/to/current/working/directory #modify this path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eQaZ1k6Tqisk","executionInfo":{"status":"ok","timestamp":1659131276720,"user_tz":420,"elapsed":458,"user":{"displayName":"Camille Noufi","userId":"04628122647747040750"}},"outputId":"9c89c5e8-05cf-4fbb-ab73-6d81df509d92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Teaching/DL4MIR2022/Notebooks\n"]}]},{"cell_type":"code","source":["# Now we will download the GTZAN dataset from Kaggle. To do this, use the following steps.\n","\n","# 1. Make a Kaggle account: https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2F\n","# 2. Go to your account, scroll to the API section. Click Expire API Token to remove previous tokens if necessary.\n","# 3. Click on Create New API Token. It will download a kaggle.json file on your machine.\n","\n","# 4. Upload the file from your machine:\n","!pip install -q kaggle\n","from google.colab import files\n","files.upload()\n","\n","# 5. make a new directory within Drive named kaggle and copy the kaggle.json file there\n","# comment the mkdir command out if you have run this cell already\n","# !rm -r ~/.kaggle\n","# !mkdir ~/.kaggle\n","!mv ./kaggle.json ~/.kaggle/\n","\n","# 6. change the permissions of the file\n","!chmod 600 ~/.kaggle/kaggle.json"],"metadata":{"id":"cp9fg1GisQh4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now we are ready to download the GTZAN dataset.\n","# YOU ONLY NEED TO RUN THIS ONCE!\n","!kaggle datasets download -d carlthome/gtzan-genre-collection --unzip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"95U6SMipuxJd","executionInfo":{"status":"ok","timestamp":1659130888621,"user_tz":420,"elapsed":578,"user":{"displayName":"Camille Noufi","userId":"04628122647747040750"}},"outputId":"f194b082-28be-457a-c36a-6f065f61ad4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/bin/kaggle\", line 5, in <module>\n","    from kaggle.cli import main\n","  File \"/usr/local/lib/python3.7/dist-packages/kaggle/__init__.py\", line 23, in <module>\n","    api.authenticate()\n","  File \"/usr/local/lib/python3.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 166, in authenticate\n","    self.config_file, self.config_dir))\n","OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"]}]},{"cell_type":"code","source":["#9. Confirm the dataset is downloaded and unzipped in the expected location:\n","# you should see a the full 'genres' folder path and a list of all the genres in the dataset\n","%cd genres\n","!ls\n","%cd ..\n","\n","# 10. Pat yourself on the back! \n","# Kaggle is a great source for open-source and competition datasets.  You can use this process to work with other datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0W0xBjtArxQZ","executionInfo":{"status":"ok","timestamp":1659131304348,"user_tz":420,"elapsed":543,"user":{"displayName":"Camille Noufi","userId":"04628122647747040750"}},"outputId":"56290af0-274c-4d61-8666-e618bb0d6a6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Teaching/DL4MIR2022/Notebooks/genres\n","blues  classical  country  disco  hiphop  jazz\tmetal  pop  reggae  rock\n","/content/drive/MyDrive/Teaching/DL4MIR2022/Notebooks\n"]}]},{"cell_type":"code","source":["# The GTZAN dataset has 1000 30-second-long \"tracks\" across 10 different musical genres\n","# There are 100 recordings for each genre.\n","\n","# Let's explore the format of the downloaded dataset.  We can look at the dataset on the Kaggle page to get an idea of the file structure:\n","#     https://www.kaggle.com/datasets/carlthome/gtzan-genre-collection\n","# The Data Explorer on the right-hand pane provides a graphical version of the file structure.\n","# We can see that each filename contains the genre and a unique number within that folder.  \n","# We can use these file names as our track ids.\n","\n","import os\n","import numpy as np\n","import librosa\n","\n","# get the 1000 different \"track_ids\" by recursing over directory and subidrectory\n","\n","def getTrackIDs(dir_name):\n","    # create a list of file and sub directories \n","    # names in the given directory \n","    file_list = os.listdir(dir_name)\n","    all_tracks = list()\n","    # Iterate over all the entries\n","    for entry in file_list:\n","        # Create full path\n","        full_path = os.path.join(dir_name, entry)\n","        # If entry is a directory then get the list of files in this directory \n","        if os.path.isdir(full_path):\n","            all_tracks = all_tracks + getTrackIDs(full_path)\n","        else:\n","            all_tracks.append(full_path)   \n","    return all_tracks\n","\n","all_tracks = getTrackIDs('./genres')\n","\n","print(\"Number of tracks: \", #your code here\n","\n","\n","# Q: Why do we want to store the filepath rather than just the filename?\n","# A: \n","\n","# It is always good to explore your data files before you begin working with them. Let's check out the structure of one of the audio files:\n","sample_id = #your code here\n","print(\"Sample track ID:\", sample_id)\n","\n","x, sr = librosa.load(#your code here\n","\n","print('\\nSignal Shape:', #your code here\n","print('Sampling Rate:', #your code here"],"metadata":{"id":"aT1sVvGHtg1C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's split these recordings into training (~85%), validation (~10%), and test (~5%) sets\n","# randomly separate these different \"track_ids\" intro training, validation, and test sets\n","\n","Ntracks = len(#your code here\n","\n","track_idx = np.random.choice(Ntracks,Ntracks,replace=False)\n","\n","tr_tracks = #your code here\n","vl_tracks = #your code here\n","ts_tracks = #your code here"],"metadata":{"id":"b-SpXaCeIn1Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To feed this data into a CNN, we must define a DataGenerator class that\n","# will create sequences of data and store them in mini batches\n","\n","import tensorflow as tf\n","\n","class DataGenerator(tf.keras.utils.Sequence):\n","    \n","    # The class constructor\n","    def __init__(\n","          self, \n","          track_ids,      # a list with the track_ids that belong to the set\n","          batch_size=32,  # the default number of datapoints in a minibatch\n","          ntime=None,     # to work with a time-frequency representation (you can work in another domain or with other features if you want)\n","          nfft=None,      # to work with a time-frequency representation (you can work in another domain or with other features if you want)\n","          n_channels=1,   # the default number of \"channels\" in the input to the CNN\n","          n_classes=10,   # the number of classes          \n","        ):\n","            \n","        self.ntime = ntime # to work with a time-frequency representation (you can work in another domain or with other features if you want)\n","        self.nfft = nfft   # to work with a time-frequency representation (you can work in another domain or with other features if you want)\n","        self.batch_size = batch_size        \n","        self.track_ids = track_ids\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes                \n","\n","    # this method returns how many batches there will be per epoch\n","    def __len__(self):\n","        '''\n","        divide the total number of datapoints in the set\n","        by the batch size. Make sure this returns an integer\n","        '''\n","        return #your code here\n","\n","    # iterates over the mini-batches by their index,\n","    # generates them, and returns them\n","    def __getitem__(self, index):\n","        \n","        # get the track ids that will be in a batch\n","        track_ids_batch = #your code here\n","\n","        # Generate data\n","        X, y = self.__data_generation(track_ids_batch)\n","\n","        return X, y\n","  \n","    # actually loads the audio files and stores them in an array \n","    def __data_generation(self, track_ids_batch):\n","        ''''\n","        the matrix with the audio data will have a shape [batch_size, ntime, nmel, n_channels] \n","        (to work with a time-frequency representation; you can work in another domain if you want)\n","        '''\n","        \n","        # Generate data\n","        X = []\n","        y = []\n","        for t in track_ids_batch:\n","            \n","            # load the file\n","            x, sr = #your code here\n","            # calculate the stft (to work with a time-frequency representation; you can work in another domain if you want)\n","            # hint: do you really need to listen 30 seconds of audio to know the genre of a popular song?\n","            x = librosa.stft(#your code here\n","            \n","            # convert to db (to work with a time-frequency representation; you can work in another domain if you want)\n","            X.append(librosa.amplitude_to_db(#your code here\n","\n","            # Store class index\n","            if 'blues' in t:\n","              y.append(0)\n","            elif 'classical' in t:\n","              y.append(1)\n","            elif 'country' in t:\n","              y.append(2)\n","            elif 'disco' in t:\n","              y.append(3)\n","            elif 'hiphop' in t:\n","              y.append(4)\n","            elif 'jazz' in t:\n","              y.append(5)\n","            elif 'metal' in t:\n","              y.append(6)\n","            elif 'pop' in t:\n","              y.append(7)\n","            elif 'reggae' in t:\n","              y.append(8)\n","            elif 'rock' in t:\n","              y.append(9)\n","            else:\n","              raise ValueError('label does not belong to valid category')\n","\n","        # return the input data batch along with the labels reformatted to be one-hot encoded vectors\n","        return np.array(X), tf.keras.utils.to_categorical(#your code here"],"metadata":{"id":"fEL94VdNSxJ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# a very simple (and bad) CNN\n","# you should make it better. This one is actually very very VERY bad\n","\n","# learning parameters\n","lr = 0.0001\n","\n","# input data and label parameters\n","ntime = 120\n","nfft = 256\n","nclasses = 10\n","\n","# declaring the input to the model\n","inputs = tf.keras.Input(shape = (ntime,1+nfft//2,1))\n","\n","# defining the CNN\n","cnn1 = tf.keras.layers.Conv2D(4, 5, activation = 'relu', padding='SAME')(inputs)\n","mxp1 = tf.keras.layers.MaxPooling2D(pool_size = 2, strides = 2, padding='SAME')(cnn1)\n","flat = tf.keras.layers.Flatten()(mxp1)\n","outputs = tf.keras.layers.Dense(10)(flat)\n","\n","bad_cnn = tf.keras.Model(inputs=inputs, outputs=outputs)\n","\n","# visualize the architecture\n","bad_cnn.summary()\n","\n","# compile the model\n","bad_cnn.compile(\n","    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n","    metrics=[\"accuracy\"],\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UpHmc-siUHpR","outputId":"0a08c4e7-544a-4ef7-9bad-165925001f46","executionInfo":{"status":"ok","timestamp":1659132928932,"user_tz":420,"elapsed":150,"user":{"displayName":"Camille Noufi","userId":"04628122647747040750"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_7 (InputLayer)        [(None, 120, 129, 1)]     0         \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 120, 129, 4)       104       \n","                                                                 \n"," max_pooling2d_6 (MaxPooling  (None, 60, 65, 4)        0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_6 (Flatten)         (None, 15600)             0         \n","                                                                 \n"," dense_6 (Dense)             (None, 10)                156010    \n","                                                                 \n","=================================================================\n","Total params: 156,114\n","Trainable params: 156,114\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# define the data generators\n","training_generator = DataGenerator(tr_tracks, ntime=ntime, nfft=nfft)\n","validation_generator = DataGenerator(vl_tracks, ntime=ntime, nfft=nfft)"],"metadata":{"id":"07hq2M9nUkQI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train the model\n","tr_logs = bad_cnn.fit(training_generator, validation_data=validation_generator, epochs=10)"],"metadata":{"id":"wqBxGm63WKub"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# after training a good CNN, do the usual visualization of the training and validation loss across epochs\n","\n","# then inspect the model's accuracy on the validation set and the confusion matrix on the validation set\n","\n","# If you do everything right and design a good CNN, you should be able to train a model that achieves\n","# over 70% accuracy on the validation set\n","\n","# If you do everything perfectly and design an outstanding CNN, you will be able to train a model that achieves\n","# 90% accuracy on the validation set.\n","\n","# When you are done, analyze the model's performance on the test set, \n","# and create a post on our subreddit sharing your model's test-set accuracy\n","# and confusion matrix"],"metadata":{"id":"1qa3LnIKqpyy"},"execution_count":null,"outputs":[]}]}