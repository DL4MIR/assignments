{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn_pytorch_base.ipynb","provenance":[{"file_id":"https://github.com/dl4genaudio/assignments/blob/main/cnn.ipynb","timestamp":1658968123934}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZfDOqYceIJnz"},"outputs":[],"source":["### Notebook for Pytorch implementation of CNN-based GTZAN classifier using Keras\n","# Potentially use as script for live coding tutorial after students have done the Keras version\n","\n","# Main Goals:\n","# 1. more practice with pytorch via a direct comparison to keras\n","# 2. training with minibatches\n","# 3. use of the Dataset and Dataloader classes to create custom datasets\n","# 4. more practice with CNN architecture design\n","# 5. more practice with training, debugging and experimentation"]},{"cell_type":"code","source":["# mount your Google drive so that you only have to download the data only once\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/path/to/notebook"],"metadata":{"id":"aKZDe3WDIWdU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now we will download the GTZAN dataset from Kaggle. To do this, use the following steps.\n","\n","# 1. Make a Kaggle account: https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2F\n","# 2. Go to your account, scroll to the API section. Click Expire API Token to remove previous tokens if necessary.\n","# 3. Click on Create New API Token. It will download a kaggle.json file on your machine.\n","\n","# 4. Upload the file from your machine:\n","!pip install -q kaggle\n","from google.colab import files\n","files.upload()\n","\n","# 5. make a new directory within Drive named kaggle and copy the kaggle.json file there.\n","# comment the mkdir command out if you have run this cell already\n","# !rm -r ~/.kaggle\n","!mkdir ~/.kaggle\n","!mv ./kaggle.json ~/.kaggle/\n","\n","# 6. change the permissions of the file\n","!chmod 600 ~/.kaggle/kaggle.json"],"metadata":{"id":"cp9fg1GisQh4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now we are ready to download the GTZAN dataset.\n","# YOU ONLY NEED TO RUN THIS ONCE!\n","!kaggle datasets download -d carlthome/gtzan-genre-collection --unzip"],"metadata":{"id":"95U6SMipuxJd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The GTZAN dataset has 1000 30-second-long \"tracks\" across 10 different musical genres\n","# There are 100 recordings for each genre.\n","\n","# Let's explore the format of the downloaded dataset.  We can look at the dataset on the Kaggle page to get an idea of the file structure:\n","#     https://www.kaggle.com/datasets/carlthome/gtzan-genre-collection\n","# The Data Explorer on the right-hand pane provides a graphical version of the file structure.\n","# We can see that each filename contains the genre and a unique number within that folder.  \n","# We can use these file names as our track ids.\n","\n","%cd genres\n","!ls\n","%cd ..\n","\n","import os\n","import numpy as np\n","import librosa\n","\n","# get the 1000 different \"track_ids\" by recursing over directory and subidrectory\n","\n","def getTrackIDs(dir_name):\n","    # create a list of file and sub directories \n","    # names in the given directory \n","    file_list = os.listdir(dir_name)\n","    all_tracks = list()\n","    # Iterate over all the entries\n","    for entry in file_list:\n","        # Create full path\n","        full_path = os.path.join(dir_name, entry)\n","        # If entry is a directory then get the list of files in this directory \n","        if os.path.isdir(full_path):\n","            all_tracks = all_tracks + getTrackIDs(full_path)\n","        else:\n","            all_tracks.append(full_path)   \n","    return all_tracks\n","\n","all_tracks = getTrackIDs('./genres')\n","\n","print(\"Number of tracks: \", len(all_tracks))\n","\n","sample_id = all_tracks[len(all_tracks)-1]\n","print(\"Sample track ID:\", sample_id)\n","\n","# It is always good to explore your data files before you begin working with them. Let's check out the structure of one of the audio files:\n","x, sr = librosa.load(sample_id, sr=None)\n","\n","print('\\nSignal Shape:', x.shape)\n","print('Sampling Rate:', sr)"],"metadata":{"id":"aT1sVvGHtg1C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's split these recordings into training (~85%), validation (~10%), and test (~5%) sets\n","# randomly separate these different \"track_ids\" intro training, validation, and test sets\n","\n","Ntracks = len(all_tracks)\n","\n","track_idx = np.random.choice(Ntracks,Ntracks,replace=False)\n","\n","tr_tracks = [all_tracks[i] for i in track_idx[:int(Ntracks*0.85)]]\n","vl_tracks = [all_tracks[i] for i in track_idx[int(Ntracks*0.85):int(Ntracks*0.95)]]\n","ts_tracks = [all_tracks[i] for i in track_idx[int(Ntracks*0.95):]]"],"metadata":{"id":"b-SpXaCeIn1Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now we will import pytorch and we will use this library\n","# to build our custom dataloader and model\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","# you may also use a GPU if you have one available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","# To feed this data into a CNN, we must define a Dataset class that\n","# will create sequences of data and store them in mini batches\n","\n","class GTZAN(Dataset):\n","    \n","    # The class constructor\n","    def __init__(\n","          self, \n","          track_ids,      # a list with the track_ids that belong to the set\n","          ntime=None,     # to work with a time-frequency representation (you can work in another domain or with other features if you want)\n","          nfft=None,      # to work with a time-frequency representation (you can work in another domain or with other features if you want)\n","          n_channels=1,   # the default number of \"channels\" in the input to the CNN\n","          n_classes=10,   # the number of classes          \n","        ):\n","            \n","        self.ntime = ntime # to work with a time-frequency representation (you can work in another domain or with other features if you want)\n","        self.nfft = nfft   # to work with a time-frequency representation (you can work in another domain or with other features if you want)\n","        self.batch_size = batch_size        \n","        self.track_ids = track_ids\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes                \n","\n","    # this method returns how many samples are in the set\n","    def __len__(self):\n","\n","        return #your code here\n","\n","    # get individual data, label pairs\n","    def __getitem__(self, index):\n","        \n","        #your code here\n","        \n","        return #your code here\n","  \n","    # actually loads the audio file and conver to a tensor \n","    def __data_generation(self, t):\n","        ''''\n","        the sample of audio data should have a shape [n_channels, ntime, nmel] \n","        (to work with a time-frequency representation; you can work in another domain if you want)\n","        '''\n","        # load the file\n","        \n","        # calculate the stft (to work with a time-frequency representation; you can work in another domain if you want)\n","        \n","        # convert to db (to work with a time-frequency representation; you can work in another domain if you want)\n","        \n","        # Store class index\n","        if 'blues' in t:\n","          y = 0\n","        elif 'classical' in t:\n","          y = 1\n","        elif 'country' in t:\n","          y = 2\n","        elif 'disco' in t:\n","          y = 3\n","        elif 'hiphop' in t:\n","          y = 4\n","        elif 'jazz' in t:\n","          y = 5\n","        elif 'metal' in t:\n","          y = 6\n","        elif 'pop' in t:\n","          y = 7\n","        elif 'reggae' in t:\n","          y = 8\n","        elif 'rock' in t:\n","          y = 9\n","        else:\n","          raise ValueError('label does not belong to valid category')\n","        \n","        #unsqueeze to add a channel dimension to work with nn.Conv2d(): https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n","        # your code here\n","\n","        return X,y"],"metadata":{"id":"fEL94VdNSxJ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# input data and label parameters\n","ntime = 120\n","nfft = 256\n","nclasses = 10\n","\n","# define the data generators and data loaders\n","train_data = GTZAN(tr_tracks, ntime=ntime, nfft=nfft)\n","val_data = GTZAN(vl_tracks, ntime=ntime, nfft=nfft)\n","\n","train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=0)\n","val_loader = DataLoader(val_data, batch_size=32, shuffle=True, num_workers=0)"],"metadata":{"id":"ycqugdBFKKSX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the pytorch version of a simple (bad) CNN\n","\n","# learning parameters\n","\n","#initialize 1) the model architecture and 2) the forward data flow through the net\n","class Model(nn.Module):\n","    # 1) Define and initialize the neural network\n","    # Hint: see: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n","    # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n","    def __init__(self, out_features, device):\n","      super(Model, self).__init__()\n","\n","      # define model parameters\n","      self.out_features = out_features\n","      self.device = device\n","      \n","      #Define the model layers\n","      self.conv = nn.Conv2d(1, 4, 5)\n","      self.relu = nn.ReLU()\n","      self.max_pooling2d = nn.MaxPool2d(2,2)\n","      self.flat = nn.Flatten()\n","      self.out = nn.Linear(14384,out_features)\n","\n","    # 2) Specify how data will pass through our model\n","    def forward(self, x):\n","      z1 = self.conv(x)\n","      o1 = self.relu(z1)\n","      o2 = self.max_pooling2d(o1)\n","      z2 = self.flat(o2)\n","      # print(z2.shape)\n","      theta = self.out(z2)\n","      return theta\n","\n","# initialize the model\n","model = Model(out_features=nclasses, device=device).float().to(device)\n","\n","# Next, we will set up the training optimization using the parameters we chose above:\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=reg) #this applies our L2 regularization\n","\n","# Then we define the loss function outside of the model class and before the training loop. \n","loss_fn = nn.CrossEntropyLoss() #softmax + negative log-likelihood\n","\n","# Then, we will create a function to compare the predicted classes and the actual classes to calculate the accuracy\n","def multi_acc(theta, Y):\n","    # apply softmax to logits to obtain class probabilities\n","    class_probs = torch.log_softmax(theta, dim = 1)\n","    # select the highest probability to be the correct class\n","    _, Yhat = torch.max(class_probs, dim = 1)    \n","    \n","    # count how many times in the batch the prediction matches the true class\n","    is_correct = (Yhat == Y).float()\n","    # convert this to a percentage\n","    accuracy = is_correct.sum() / len(is_correct)\n","    return accuracy, Yhat\n","\n","\n","############## Model Information #####################################\n","# Lastly we will print out the summary of our model\n","print(f\"Model structure: {model}\\n\")\n","for name, param in model.named_parameters():\n","    print(f\"Layer: {name} | Size: {param.size()}\")\n","\n","# total parameters and trainable parameters\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"\\n{total_params:,} total parameters.\")\n","total_trainable_params = sum(\n","    p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"{total_trainable_params:,} training parameters.\\n\")"],"metadata":{"id":"UpHmc-siUHpR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now that the model has been built, let's see how it performs on the validation data\n","# before being trained.\n","\n","# Q: why do we use the torch.no_grad() method here?\n","# A:\n","# Q: does the evaluation before training agree with your expectation?  Why does this initial loss value make sense? Hint: Think about the log-likehood equation.\n","# A: \n","\n","#sample pass\n","loss = 0.0\n","acc = 0.0\n","with torch.no_grad():\n","  for batch in val_loader:\n","    X, y = batch\n","    theta = model(#your code here)\n","    loss += loss_fn(theta, #your code here)\n","    batch_acc,_ = multi_acc(theta, #your code here)\n","    acc += batch_acc\n","\n","  print(f'Loss: {#your code here}')"],"metadata":{"id":"wqBxGm63WKub"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now we can move on to train the model\n","# we will add 2 inner for-loops to our training now that we are dealing with our data in mini batches\n","# thus, we will also need to make sure we track our batch loss appropriately\n","\n","# define 2 dictionaries which will store the accuracy/epoch and loss/epoch for both train and validation sets\n","accuracy_stats = {\n","    'train': [],\n","    \"val\": []\n","}\n","loss_stats = {\n","    'train': [],\n","    \"val\": []\n","}\n","\n","############## Main Training Loop ###################\n","for epoch in range(nepochs):\n","\n","  train_loss = 0\n","  train_acc = 0\n","  val_loss = 0\n","  val_acc = 0\n","\n","  for i, batch in enumerate(train_loader):\n","      \n","      X,y = batch\n","\n","      # zero the epoch loss and parameter gradients\n","      optimizer.zero_grad()\n","      \n","      # Training: forward + backward + optimize\n","      theta = model(X.to(device))\n","      loss = loss_fn(theta, y.to(device))\n","      acc,Yhat = multi_acc(theta, y.to(device))\n","      \n","      loss.backward()\n","      optimizer.step()\n","\n","      train_loss += loss.item()/len(train_loader)\n","      train_acc += acc/len(train_loader)\n","\n","  for i, batch in enumerate(val_loader):\n","      X,y = batch\n","      # Validation: forward only\n","      with torch.no_grad():\n","        # send validation data through the network\n","        theta = model(X.to(device))\n","        loss = loss_fn(theta, y.to(device))\n","        acc,Yhat = multi_acc(theta, y.to(device))\n","        val_loss += loss.item()/len(val_loader)\n","        val_acc += acc/len(val_loader)\n","\n","  # Save training stats:\n","  loss_stats['train'].append(train_loss)\n","  loss_stats['val'].append(val_loss)\n","  accuracy_stats['train'].append(train_acc)\n","  accuracy_stats['val'].append(val_acc)\n","\n","  # print statistics\n","  print(f'Epoch {epoch+1}: Train Loss: {train_loss:.3f} | Train Acc.: {train_acc:.4f} | Val Loss: {val_loss:.3f} | Val Acc.: {val_acc:.4f}')\n","\n","print('Finished Training')"],"metadata":{"id":"kofK1OFQi74Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now that training is done, let's visualize the training and validation loss\n","# all of that information is readily available in the \"training logs\"\n","\n","import matplotlib.pyplot as plt\n","\n","# summarize history for loss\n","plt.plot(loss_stats['train'])\n","plt.plot(loss_stats['val'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper right')\n","plt.show()"],"metadata":{"id":"bcvksvMxlpR8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Questions:\n","# What are some training peculiarities you notice after training this simple (bad) CNN?\n","# A: \n","# List a couple ways you could remedy some of these peculiarities.\n","# A: modify epoch loss, add dropout, modify the learning rate, add in early stopping criterion, etc\n","\n","# Next, keeping in mind your answers above and some of the other training techniques we have learned so far,\n","# improve the performance of the above CNN\n","\n","# after training a good CNN, do the usual visualization of the training and validation loss across epochs\n","\n","# then inspect the new model's accuracy on the validation set and the confusion matrix on the validation set\n","\n","# If you do everything right and design a good CNN, you should be able to train a model that achieves\n","# over 70% accuracy on the validation set\n","\n","# If you do everything perfectly and design an outstanding CNN, you will be able to train a model that achieves\n","# 90% accuracy on the validation set.\n","\n","# When you are done, analyze the model's performance on the test set, \n","# and create a post on our discord sharing your model's test-set accuracy\n","# and confusion matrix\n","\n","# you can likely re-use much of your visualization code from the previous CNN notebook"],"metadata":{"id":"1qa3LnIKqpyy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"LQyY-aoRsjEa"},"execution_count":null,"outputs":[]}]}