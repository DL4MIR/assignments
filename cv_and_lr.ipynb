{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cv_and_lr.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1MNjfrY7toSK"},"outputs":[],"source":["# In this notebook we will get familiar with cross-validation, linear \n","# regression, and gradient descent optimization\n","\n","# We will continue working with the tinysol dataset\n","\n","# install mirdata on the colab shell\n","### your code here\n","\n","# now import mirdata\n","### your code here"]},{"cell_type":"code","source":["# now we can initialize the tinysol dataset and (down)load it\n","tinysol = ### your code here\n","tinysol### your code here"],"metadata":{"id":"CiFKhQVxtqB-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# let's load a few libraries that we will need\n","import librosa\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","###############################################################\n","# To make algorithm optimization easy, we will work with two  #\n","# features (i.e. two dimensions) so that we are               #\n","# able to visualize linear regression and its optimization    #\n","#                                                             #\n","# we will extract a couple features from ALL datapoints       #\n","# that we want to work with                                   #\n","###############################################################\n","\n","# get all the track_ids\n","all_tracks = ### your code here\n","\n","# the datapoints that we want to work with are ALL the Viola tones\n","\n","# create a \"data\" matrix, where rows correspond to each viola datapoint\n","# \"data\" has two columns, the first one with the f0 and the second one\n","# with the spectral centroid. Use librosa to extract these features\n","# from each Viola track in the tinysol dataset. To extract the f0 (fundamental\n","# frequency), use librosa.yin (make sure you use all the correct arguments\n","# for this function; use of the correct arguments is VERY IMPORTANT).\n","\n","data = []\n","for t in all_tracks:  \n","  if tinysol.track(t).instrument_abbr == 'Va':        \n","    x,sr = ### your code here to load the audio and sr\n","    # feature 1. f0\n","    f0 = ### your code here\n","    # feature 2. spectral centroid\n","    sc = ### your code here\n","    \n","    # concatenate the two features from each datapoint\n","    data.append([f0,sc])\n","\n","data = np.array(data)\n","\n","print(\"\\nThe shape of 'data' is \", data.shape,\"\\n\")\n","\n","# we can also visualize these features\n","plt.scatter(data[:,0],data[:,1])\n","plt.xlabel('f0 (Hz)')\n","plt.ylabel('spectral centroid')\n","plt.show()"],"metadata":{"id":"H-ZYUUnCUxaj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now randomly select ~5% of the data and set it apart as test set\n","# Hint: you can use the np.random.choice function (with replace=False) \n","# and use the first ~5% of its output to index out the test set\n","\n","all_idx = ### your code here\n","\n","data_ts = data[all_idx[### your code here\n","data_tr = data[all_idx[### your code here\n","\n","print(\"The shape of the training data is \", data_tr.shape)\n","print(\"The shape of the testing data is \", data_ts.shape)"],"metadata":{"id":"hZvqBkhZt-BQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some libraries we will use\n","import time\n","! pip install sklearn\n","from sklearn.model_selection import KFold\n","from IPython import display\n","\n","\n","# now let's standardize our training features to be zero-mean and unit variance\n","mu_tr = ### your code here\n","std_tr = ### your code here\n","data_tr = ### your code here"],"metadata":{"id":"8yuhIy7iw2h7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Here's the main body of this homework\n","\n","# we will use k-fold cross validation to optimize a simple linear regression\n","# model using gradient descent. \n","\n","# let's start by defining our independent (x) and dependent (y) variables\n","# Q: which one of the two features (f0 or spectral centroid) should be the\n","# independent variable? Why?\n","# A: \n","x = ### your code here\n","y = ### your code here\n","\n","# Now define the learning variables for the optimization algorithm\n","epochs = ### your code here to define the number of epochs\n","lr = ### your code here to define the learning rate\n","\n","# we are ready to start the optimization routine\n","# using sklearn.Kfold, we will split the data into 5 folds\n","# and we will use each fold as validation set in a for loop\n","n_splits = 5\n","kf = KFold(n_splits=n_splits)\n","# we will also save the slope and bias terms that we find\n","# for each fold in a list\n","ws = []\n","bs = []\n","all_Jtrs = []\n","all_Jvls = []\n","for ifold, (tr_idx, vl_idx) in enumerate(kf.split(x)):\n","\n","  print(\"\\n#######################\")\n","  print(\"# training fold No. {} #\".format(ifold+1))\n","  print(\"#######################\\n\")\n","  time.sleep(3)\n","\n","  # organize the data into training and validation splits\n","  x_tr, x_vl = x[tr_idx], x[vl_idx]\n","  y_tr, y_vl = y[tr_idx], y[vl_idx]\n","\n","  # intialize the slope and bias as\n","  # random numbers drawn from a normal\n","  # distribution (use np.random.randn)\n","  w = ### your code here\n","  b = ### your code here\n","\n","  # we will repeatedly show the data \n","  # to out gradient descent\n","  # algorithm a few times \n","  Jtrs = []\n","  Jvls = []\n","  for e in range(epochs):\n","    \n","    # compute y_hat with the training data\n","    y_hat_tr = ### your code here\n","    # compute y_hat with the validation data\n","    y_hat_vl = ### your code here\n","\n","    # compute the loss function with the training data\n","    J_tr = ### your code here\n","    # compute the loss function with the validation data\n","    J_vl = ### your code here\n","\n","    # save the training and validation loss to visualize at the end\n","    Jtrs.append(J_tr)\n","    Jvls.append(J_vl)\n","\n","    print(\"at epoch {} the training loss J_tr = {:.5f}\".format(e+1,J_tr))\n","    print(\"          and validation loss J_vl = {:.5f}\".format(J_vl))\n","    print(\"    w = \", w)\n","    print(\"    b = \", b)\n","    \n","    \n","    # now let's plot to see our learning progress every epoch\n","    time.sleep(1.5)\n","    plt.scatter(x_tr,y_tr,c='blue')\n","    plt.xlabel('f0 (standardized)')\n","    plt.ylabel('spectral centroid (standardized)')\n","    plt.plot([np.min(x_tr),np.max(x_tr)], [np.min(x_tr)*w+b,np.max(x_tr)*w+b],c='red') if e== 19 else plt.plot([np.min(x_tr),np.max(x_tr)], [np.min(x_tr)*w+b,np.max(x_tr)*w+b],c='black',linestyle='dashed',linewidth=0.2)    \n","    display.clear_output(wait=True)\n","    display.display(plt.gcf())        \n","\n","    # now compute the gradient of w and b\n","    dw = ### your code here\n","    db = ### your code here\n","\n","    # and update w and b\n","    w = w - lr*dw\n","    b = b - lr*db\n","  \n","  display.clear_output()\n","\n","  # let's save the fold's optimized parameters and each epoch tr and vl loss\n","  ws.append(w)\n","  bs.append(b)\n","  all_Jtrs.append(Jtrs)\n","  all_Jvls.append(Jvls)\n","\n","for i in range(n_splits):\n","  print(\"for fold No.\",i+1, \", the optimized parameters were\", i, \" w=\", ws[i], \"and b=\",bs[i])"],"metadata":{"id":"6vZvfsEnhcRX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this cell works without needing to change or add code\n","\n","print(\"\\nPlotting the training and validation losses:\\n\")\n","\n","plt.plot(np.array(all_Jtrs).T)\n","plt.title('Training loss (each line is a different fold)')\n","plt.xlabel('Epoch No.')\n","plt.ylabel('J')\n","plt.ylim([0,3])\n","plt.show()\n","plt.plot(np.array(all_Jvls).T)\n","plt.title('Validation loss (each line is a different fold)')\n","plt.xlabel('Epoch No.')\n","plt.ylabel('J')\n","plt.ylim([0,3])\n","plt.show()\n","plt.title('Loss (mean across folds)')\n","plt.plot(np.mean(np.array(all_Jtrs),axis=0),label='Training')\n","plt.plot(np.mean(np.array(all_Jvls),axis=0),label='Validation')\n","plt.xlabel('Epoch No.')\n","plt.ylabel('J')\n","plt.ylim([0,1.6])\n","plt.legend()\n","plt.show()\n","\n","# Q: how do the losses differ between folds? why do you think this is seen?\n","# A:\n","\n","# Q: how do the losses differ between training and validation splits? why do you think this is seen?\n","# A:\n","\n","# Q: is your model overfit, underfit, or properly optimized? how do you know this?\n","# A:"],"metadata":{"id":"z9QbDu7H94pX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["####################\n","# model evaluation #\n","####################\n","\n","# now you have a list of 'w' and a list of 'b' parameters\n","# Q: why do we have many 'w' and many 'b', if we only need one of each for our model?\n","# A:\n","\n","# to evaluate the model, we only need one, not five. \n","# Q: how would you go about getting a single 'w' and a single 'b'?\n","# A:\n","\n","best_w = ### your code here\n","best_b = ### your code here\n","\n","# Now, using your optimized 'w' and 'b' calculate the absolute error between y_hat and y with ALL your training\n","training_error = ### your code here\n","print(\"The standardized training error was: \", training_error)\n","\n","# Q: why do we say that this is the \"standardized training\" error?\n","# A:\n","\n","# Q: should we consider calculating the non-standardized training error? why?\n","# A: \n","\n","# if you are happy with your model optimization, you may move on to the next cell. \n","\n","##################################################################################\n","# ATTENTION: DO NOT move to the next cell until you have a well-optimized model. #\n","# continuing without a well-optimized model will ruin the model evaluation.      #\n","##################################################################################"],"metadata":{"id":"DviYF-_vvwuL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##################################################################\n","# RUN THIS CELL ONLY ONCE WHEN YOU ARE DONE WITH THE CELLS ABOVE #\n","# Running this cell more than once is a violation of             #\n","# international Machine Learning Law                             #\n","##################################################################\n","# Q: why is it so important that you run this cell only once at the\n","# very end?\n","# A:\n","\n","\n","# now do the same evaluation with the test set\n","\n","# IMPORTANT: you will have to standardize the test data using the training data mu and std\n","# Q: why do we standardize the test data with the training mu and std?\n","# A:\n","\n","data_ts = ### your code here\n","\n","x_ts = ### your code here\n","y_ts = ### your code here\n","\n","test_set_error = ### your code here\n","print(\"The standardized testing error was: \", test_set_error)\n","plt.scatter(x_ts,y_ts,c='blue')\n","plt.plot([np.min(x_ts),np.max(x_ts)], [np.min(x_ts)*best_w+best_b,np.max(x_ts)*best_w+best_b],c='red')\n","plt.xlabel('f0 (standardized)')\n","plt.ylabel('spectral centroid (standardized)')\n","plt.title('Regression model on the test set')    \n","plt.show()"],"metadata":{"id":"GN812LVcx0CM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now your homework truly starts: (just kidding, you are almost done this time and there is no more \"coding\")\n","\n","# Q: were your optimized 'w' and 'b' parameters good for the test set data? how do you know if they were or not?\n","# A: \n","\n","# Q: which was lower, the training or the test error? Why?\n","# A: \n","\n","# Q: Would the same 'w' and 'b' parameters work with another musical instrument?\n","# A: \n","\n","# Q: Would the same 'w' and 'b' parameters work with a new recording of Viola tones?\n","# A: \n","\n","# Q: \n","# Picture this scenario: your friend has arranged Morton Feldman's \"the viola\n","# in my life\" to be played with solo viola. She wants to create a visualization \n","# of the viola spectral centroid (she will map the spectral centroid values to \n","# a heatmap of colors that is projected on a screen behind her, but the exact details\n","# are a secret to surprise the public and she does not want to tell you more about it) \n","#\n","# She has a problem though, and you must help her:\n","#\n","# from the score that she will follow, she can only get f0 for each tone, and she also needs the spectral centroid. \n","#\n","# What would you have to do to use your linear regression model for \n","# her live performance, predicting the spectral centroid from the f0 data in the score?\n","# \n","# Be as detailed as possible explaining the steps to do f0 data pre-processing\n","# and prediction usng your model on \"unseen, real-world data\"\n","#\n","# A:\n","#\n","#\n","#\n","#\n","# Q: which short-comings do you anticipate your model will face when being\n","# applied on this real-world data? Is your model good enough to do this in the first place?\n","# how do you know?\n","# A:\n","#\n","#\n","# Remember: to successfully help her, she will give you f0 values, and you\n","# must give her the corresponding spectral-centroid values, all of this BEFORE\n","# the live performance. In other words, you do NOT have access to audio data AT ALL.\n","# You only have the score and the f0 values."],"metadata":{"id":"8EmmdNjn3cdh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"7D6heKg7_v9V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"TVJ2QGMmKOs4"},"execution_count":null,"outputs":[]}]}